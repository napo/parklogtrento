{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42b0f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b06ae16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARKS_GEOPARQUET = \"data\" + os.sep + \"parks.geoparquet\"\n",
    "ZONES_GEOPARQUET = \"data\" + os.sep + \"zones.geoparquet\"\n",
    "# Salvataggio dei file JSON\n",
    "DEST = \"docs\" + os.sep + \"curiosity\" + os.sep + \"data\" + os.sep\n",
    "if not os.path.exists(DEST):\n",
    "    os.makedirs(DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e128e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLIDAYS = [\n",
    "    (1, 1),   # Capodanno\n",
    "    (1, 6),   # Epifania\n",
    "    (4, 25),  # Festa della Liberazione\n",
    "    (5, 1),   # Festa dei Lavoratori\n",
    "    (6, 2),   # Festa della Repubblica\n",
    "    (6,26),  # San Vigilio\n",
    "    (8, 15),  # Ferragosto\n",
    "    (11, 1),  # Ognissanti\n",
    "    (12, 8),  # Immacolata Concezione\n",
    "    (12, 25), # Natale\n",
    "    (12, 26)  # Santo Stefano\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b8e01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare le date mobili (Pasqua e Lunedì dell'Angelo)\n",
    "def calculate_easter(year):\n",
    "    \"\"\"Calcolo della data di Pasqua (algoritmo di Oudin 1940)\"\"\"\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * l) // 451\n",
    "    month = (h + l - 7 * m + 114) // 31\n",
    "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
    "    return datetime(year, month, day).date()\n",
    "\n",
    "def get_holiday_dates(year):\n",
    "    dates = [datetime(year, month, day).date() for month, day in HOLIDAYS]\n",
    "    easter = calculate_easter(year)\n",
    "    easter_monday = easter.replace(day=easter.day + 1)\n",
    "    dates.append(easter)\n",
    "    dates.append(easter_monday)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43791195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename):\n",
    "    path = os.path.join(DEST, filename)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcddf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = gpd.read_parquet(PARKS_GEOPARQUET)\n",
    "zones = gpd.read_parquet(ZONES_GEOPARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d986920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(df_originale, tolleranza_minuti=5, limit_fill=1, timezone=\"Europe/Rome\"):\n",
    "    # Copia iniziale\n",
    "    df = df_originale.copy()\n",
    "    df = df.drop(columns=[col for col in ['hour', 'minute'] if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Parsing timestamp e localizzazione timezone\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_convert(timezone)\n",
    "\n",
    "    # Calcolo colonne\n",
    "    df['occupied'] = df['capacity'] - df['freeslots']\n",
    "    df['percent_occupied'] = df['occupied'] / df['capacity']\n",
    "\n",
    "    # Impostiamo l'indice\n",
    "    df = df.set_index('timestamp')\n",
    "\n",
    "    # Tutti i timestamp unici\n",
    "    timestamps = df.index.unique()\n",
    "    names = df['name'].unique()\n",
    "\n",
    "    # Costruzione combinazioni timestamp x name\n",
    "    full_index = pd.MultiIndex.from_product([timestamps, names], names=[\"timestamp\", \"name\"])\n",
    "    full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "\n",
    "    # Merge iniziale\n",
    "    df_reset = df.reset_index()\n",
    "    merged = pd.merge(full_df, df_reset, on=['timestamp', 'name'], how='left')\n",
    "\n",
    "    # Riempimento ±tolleranza_minuti per ogni name\n",
    "    merged['timestamp'] = pd.to_datetime(merged['timestamp'])\n",
    "    new_dfs = []\n",
    "\n",
    "    for name in names:\n",
    "        df_name = df_reset[df_reset['name'] == name].sort_values('timestamp')\n",
    "        target = merged[merged['name'] == name].sort_values('timestamp')\n",
    "\n",
    "        merged_asof = pd.merge_asof(\n",
    "            target,\n",
    "            df_name,\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta(minutes=tolleranza_minuti),\n",
    "            suffixes=('', '_filled')\n",
    "        )\n",
    "\n",
    "        #for col in ['capacity', 'freeslots', 'occupied', 'percent_occupied']:\n",
    "        #    merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
    "        for col in ['capacity', 'freeslots', 'occupied', 'percent_occupied']:\n",
    "            if not merged_asof[f\"{col}_filled\"].isna().all() or not merged_asof[col].isna().all():\n",
    "                merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
    "\n",
    "        merged_asof = merged_asof[['timestamp', 'name', 'capacity', 'freeslots', 'occupied', 'percent_occupied']]\n",
    "        new_dfs.append(merged_asof)\n",
    "    df_filled = pd.concat(new_dfs)\n",
    "    df_filled = df_filled.set_index(['timestamp', 'name']).sort_index()\n",
    "\n",
    "    # Gestione duplicati: facciamo la media\n",
    "    df_filled = df_filled.groupby(['timestamp', 'name']).mean()\n",
    "\n",
    "    # Forward fill e backward fill limitato\n",
    "    to_fill = df_filled[['capacity', 'freeslots', 'occupied', 'percent_occupied']]\n",
    "    filled = (\n",
    "        to_fill\n",
    "        .groupby('name', group_keys=False)\n",
    "        .apply(lambda g: g.ffill(limit=limit_fill).bfill(limit=limit_fill))\n",
    "    )\n",
    "\n",
    "    df_filled[['capacity', 'freeslots', 'occupied', 'percent_occupied']] = filled\n",
    "\n",
    "    # Impostiamo i tipi finali\n",
    "    df_filled['capacity'] = df_filled['capacity'].round().astype('Int64')\n",
    "    df_filled['freeslots'] = df_filled['freeslots'].round().astype('Int64')\n",
    "    df_filled['occupied'] = df_filled['occupied'].round().astype('Int64')\n",
    "    df_filled['percent_occupied'] = df_filled['percent_occupied'].round(4)\n",
    "\n",
    "    return df_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53a000ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_park = parks[parks['type'] == 'park']\n",
    "parks_park = parks_park[['timestamp', 'name', 'capacity', 'freeslots']]\n",
    "parks_park_filled = fill_data(parks_park).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50f6dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_148845/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n"
     ]
    }
   ],
   "source": [
    "parks_bike = parks[parks['type'] == 'bike']\n",
    "parks_bike = parks_bike[['timestamp', 'name', 'capacity', 'freeslots']]\n",
    "parks_bike_filled = fill_data(parks_bike).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdffc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_blu = zones[['ts', 'name', 'stall_blu_capacity', 'stall_blu_freeslots']].copy()\n",
    "zones_blu.rename(columns={'ts': 'timestamp','stall_blu_capacity': 'capacity', 'stall_blu_freeslots': 'freeslots'}, inplace=True)\n",
    "max_capacity_per_name = zones_blu.groupby('name')['capacity'].max()\n",
    "zones_blu['capacity'] = zones_blu['name'].map(max_capacity_per_name)\n",
    "zones_blu_filled = fill_data(zones_blu).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88e21beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top_bottom_occupancy(df, category, top_n=3):\n",
    "    result = []\n",
    "    occupancy = df.groupby('name')['percent_occupied'].mean().sort_values(ascending=False)\n",
    "    top = occupancy.head(top_n)\n",
    "    bottom = occupancy.tail(top_n)\n",
    "\n",
    "    for name, occ in top.items():\n",
    "        result.append({\"name\": name, \"average_occupancy\": round(occ, 4), \"type\": \"top\"})\n",
    "    for name, occ in bottom.items():\n",
    "        result.append({\"name\": name, \"average_occupancy\": round(occ, 4), \"type\": \"bottom\"})\n",
    "\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"top_bottom_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_weekday_occupancy(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['weekday'] = df['timestamp'].dt.day_name()\n",
    "    occupancy = df.groupby('weekday')['percent_occupied'].mean()\n",
    "    occupancy = occupancy.reindex([\n",
    "        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "    ])\n",
    "\n",
    "    result = [{\"weekday\": day, \"average_occupancy\": round(occ, 4)} for day, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"weekday_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_hourly_occupancy(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    occupancy = df.groupby('hour')['percent_occupied'].mean()\n",
    "\n",
    "    result = [{\"hour\": int(hour), \"average_occupancy\": round(occ, 4)} for hour, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"hourly_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_weekend_holiday_comparison(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "    years = df['timestamp'].dt.year.unique()\n",
    "    all_holidays = []\n",
    "    for year in years:\n",
    "        all_holidays.extend(get_holiday_dates(year))\n",
    "\n",
    "    df['day_type'] = 'Weekday'\n",
    "    df.loc[df['weekday'] >= 5, 'day_type'] = 'Weekend'\n",
    "    df.loc[df['date'].isin(all_holidays), 'day_type'] = 'Holiday'\n",
    "\n",
    "    occupancy = df.groupby('day_type')['percent_occupied'].mean()\n",
    "\n",
    "    result = [{\"day_type\": dtype, \"average_occupancy\": round(occ, 4)} for dtype, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"weekend_holiday_comparison_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_turnover_parks(df, category, top_n=3):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df_sorted = df.sort_values(by=['name', 'timestamp'])\n",
    "\n",
    "    turnover = {}\n",
    "    for name, group in df_sorted.groupby('name'):\n",
    "        diffs = group['occupied'].diff().abs()\n",
    "        turnover[name] = diffs.sum()\n",
    "\n",
    "    turnover_series = pd.Series(turnover).sort_values(ascending=False)\n",
    "    top_turnover = turnover_series.head(top_n)\n",
    "\n",
    "    result = [{\"name\": name, \"total_turnover\": int(turn)} for name, turn in top_turnover.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"turnover_parks_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "# Funzioni per generare tutti i JSON per ogni categoria\n",
    "def generate_all_json_for_category(df, category):\n",
    "    generate_top_bottom_occupancy(df, category)\n",
    "    generate_weekday_occupancy(df, category)\n",
    "    generate_hourly_occupancy(df, category)\n",
    "    generate_weekend_holiday_comparison(df, category)\n",
    "    generate_turnover_parks(df, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_json_for_category(parks_park_filled, 'park')\n",
    "generate_all_json_for_category(parks_bike_filled, 'bike')\n",
    "generate_all_json_for_category(zones_blu_filled, 'zones_blu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
