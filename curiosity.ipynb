{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42b0f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calplot\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06ae16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARKS_GEOPARQUET = \"data\" + os.sep + \"parks.geoparquet\"\n",
    "ZONES_GEOPARQUET = \"data\" + os.sep + \"zones.geoparquet\"\n",
    "# Salvataggio dei file JSON\n",
    "DEST = \"docs\" + os.sep + \"curiosity\" + os.sep + \"data\" + os.sep\n",
    "if not os.path.exists(DEST):\n",
    "    os.makedirs(DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e128e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLIDAYS = [\n",
    "    (1, 1),   # Capodanno\n",
    "    (1, 6),   # Epifania\n",
    "    (4, 25),  # Festa della Liberazione\n",
    "    (5, 1),   # Festa dei Lavoratori\n",
    "    (6, 2),   # Festa della Repubblica\n",
    "    (6,26),  # San Vigilio\n",
    "    (8, 15),  # Ferragosto\n",
    "    (11, 1),  # Ognissanti\n",
    "    (12, 8),  # Immacolata Concezione\n",
    "    (12, 25), # Natale\n",
    "    (12, 26)  # Santo Stefano\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8e01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare le date mobili (Pasqua e LunedÃ¬ dell'Angelo)\n",
    "def calculate_easter(year):\n",
    "    \"\"\"Calcolo della data di Pasqua (algoritmo di Oudin 1940)\"\"\"\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * l) // 451\n",
    "    month = (h + l - 7 * m + 114) // 31\n",
    "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
    "    return datetime(year, month, day).date()\n",
    "\n",
    "def get_holiday_dates(year):\n",
    "    dates = [datetime(year, month, day).date() for month, day in HOLIDAYS]\n",
    "    easter = calculate_easter(year)\n",
    "    easter_monday = easter.replace(day=easter.day + 1)\n",
    "    dates.append(easter)\n",
    "    dates.append(easter_monday)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43791195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename):\n",
    "    path = os.path.join(DEST, filename)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcddf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = gpd.read_parquet(PARKS_GEOPARQUET)\n",
    "zones = gpd.read_parquet(ZONES_GEOPARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d986920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(df_originale, tolleranza_minuti=5, limit_fill=1, timezone=\"Europe/Rome\"):\n",
    "    # Copia iniziale\n",
    "    df = df_originale.copy()\n",
    "    df = df.drop(columns=[col for col in ['hour', 'minute'] if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Parsing timestamp e localizzazione timezone\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_convert(timezone)\n",
    "\n",
    "    # Calcolo colonne\n",
    "    df['occupied'] = df['capacity'] - df['freeslots']\n",
    "    df['percent_occupied'] = df['occupied'] / df['capacity']\n",
    "\n",
    "    # Impostiamo l'indice\n",
    "    df = df.set_index('timestamp')\n",
    "\n",
    "    # Tutti i timestamp unici\n",
    "    timestamps = df.index.unique()\n",
    "    names = df['name'].unique()\n",
    "\n",
    "    # Costruzione combinazioni timestamp x name\n",
    "    full_index = pd.MultiIndex.from_product([timestamps, names], names=[\"timestamp\", \"name\"])\n",
    "    full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "\n",
    "    # Merge iniziale\n",
    "    df_reset = df.reset_index()\n",
    "    merged = pd.merge(full_df, df_reset, on=['timestamp', 'name'], how='left')\n",
    "\n",
    "    # Riempimento Â±tolleranza_minuti per ogni name\n",
    "    merged['timestamp'] = pd.to_datetime(merged['timestamp'])\n",
    "    new_dfs = []\n",
    "\n",
    "    for name in names:\n",
    "        df_name = df_reset[df_reset['name'] == name].sort_values('timestamp')\n",
    "        target = merged[merged['name'] == name].sort_values('timestamp')\n",
    "\n",
    "        merged_asof = pd.merge_asof(\n",
    "            target,\n",
    "            df_name,\n",
    "            on='timestamp',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta(minutes=tolleranza_minuti),\n",
    "            suffixes=('', '_filled')\n",
    "        )\n",
    "\n",
    "        #for col in ['capacity', 'freeslots', 'occupied', 'percent_occupied']:\n",
    "        #    merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
    "        for col in ['capacity', 'freeslots', 'occupied', 'percent_occupied']:\n",
    "            if not merged_asof[f\"{col}_filled\"].isna().all() or not merged_asof[col].isna().all():\n",
    "                merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
    "\n",
    "        merged_asof = merged_asof[['timestamp', 'name', 'capacity', 'freeslots', 'occupied', 'percent_occupied']]\n",
    "        new_dfs.append(merged_asof)\n",
    "    df_filled = pd.concat(new_dfs)\n",
    "    df_filled = df_filled.set_index(['timestamp', 'name']).sort_index()\n",
    "\n",
    "    # Gestione duplicati: facciamo la media\n",
    "    df_filled = df_filled.groupby(['timestamp', 'name']).mean()\n",
    "\n",
    "    # Forward fill e backward fill limitato\n",
    "    to_fill = df_filled[['capacity', 'freeslots', 'occupied', 'percent_occupied']]\n",
    "    filled = (\n",
    "        to_fill\n",
    "        .groupby('name', group_keys=False)\n",
    "        .apply(lambda g: g.ffill(limit=limit_fill).bfill(limit=limit_fill))\n",
    "    )\n",
    "\n",
    "    df_filled[['capacity', 'freeslots', 'occupied', 'percent_occupied']] = filled\n",
    "\n",
    "    # Impostiamo i tipi finali\n",
    "    df_filled['capacity'] = df_filled['capacity'].round().astype('Int64')\n",
    "    df_filled['freeslots'] = df_filled['freeslots'].round().astype('Int64')\n",
    "    df_filled['occupied'] = df_filled['occupied'].round().astype('Int64')\n",
    "    df_filled['percent_occupied'] = df_filled['percent_occupied'].round(4)\n",
    "\n",
    "    return df_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53a000ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_park = parks[parks['type'] == 'park']\n",
    "parks_park = parks_park[['timestamp', 'name', 'capacity', 'freeslots']]\n",
    "parks_park_filled = fill_data(parks_park).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50f6dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n",
      "/tmp/ipykernel_78199/1698176314.py:49: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_asof[col] = merged_asof[f\"{col}_filled\"].combine_first(merged_asof[col])\n"
     ]
    }
   ],
   "source": [
    "parks_bike = parks[parks['type'] == 'bike']\n",
    "parks_bike = parks_bike[['timestamp', 'name', 'capacity', 'freeslots']]\n",
    "parks_bike_filled = fill_data(parks_bike).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdffc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_blu = zones[['ts', 'name', 'stall_blu_capacity', 'stall_blu_freeslots']].copy()\n",
    "zones_blu.rename(columns={'ts': 'timestamp','stall_blu_capacity': 'capacity', 'stall_blu_freeslots': 'freeslots'}, inplace=True)\n",
    "max_capacity_per_name = zones_blu.groupby('name')['capacity'].max()\n",
    "zones_blu['capacity'] = zones_blu['name'].map(max_capacity_per_name)\n",
    "zones_blu_filled = fill_data(zones_blu).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3316cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_bike_filled.to_csv(\"/tmp/parks_bike_filled.csv\", index=False)\n",
    "parks_park_filled.to_csv(\"/tmp/parks_park_filled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e21beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top_bottom_occupancy(df, category, top_n=3):\n",
    "    result = []\n",
    "    occupancy = df.groupby('name')['percent_occupied'].mean().sort_values(ascending=False)\n",
    "    top = occupancy.head(top_n)\n",
    "    bottom = occupancy.tail(top_n)\n",
    "\n",
    "    for name, occ in top.items():\n",
    "        result.append({\"name\": name, \"average_occupancy\": round(occ, 4), \"type\": \"top\"})\n",
    "    for name, occ in bottom.items():\n",
    "        result.append({\"name\": name, \"average_occupancy\": round(occ, 4), \"type\": \"bottom\"})\n",
    "\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"top_bottom_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_weekday_occupancy(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['weekday'] = df['timestamp'].dt.day_name()\n",
    "    occupancy = df.groupby('weekday')['percent_occupied'].mean()\n",
    "    occupancy = occupancy.reindex([\n",
    "        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "    ])\n",
    "\n",
    "    result = [{\"weekday\": day, \"average_occupancy\": round(occ, 4)} for day, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"weekday_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_hourly_occupancy(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    occupancy = df.groupby('hour')['percent_occupied'].mean()\n",
    "\n",
    "    result = [{\"hour\": int(hour), \"average_occupancy\": round(occ, 4)} for hour, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"hourly_occupancy_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_weekend_holiday_comparison(df, category):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "    years = df['timestamp'].dt.year.unique()\n",
    "    all_holidays = []\n",
    "    for year in years:\n",
    "        all_holidays.extend(get_holiday_dates(year))\n",
    "\n",
    "    df['day_type'] = 'Weekday'\n",
    "    df.loc[df['weekday'] >= 5, 'day_type'] = 'Weekend'\n",
    "    df.loc[df['date'].isin(all_holidays), 'day_type'] = 'Holiday'\n",
    "\n",
    "    occupancy = df.groupby('day_type')['percent_occupied'].mean()\n",
    "\n",
    "    result = [{\"day_type\": dtype, \"average_occupancy\": round(occ, 4)} for dtype, occ in occupancy.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"weekend_holiday_comparison_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "def generate_turnover_parks(df, category, top_n=3):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df_sorted = df.sort_values(by=['name', 'timestamp'])\n",
    "\n",
    "    turnover = {}\n",
    "    for name, group in df_sorted.groupby('name'):\n",
    "        diffs = group['occupied'].diff().abs()\n",
    "        turnover[name] = diffs.sum()\n",
    "\n",
    "    turnover_series = pd.Series(turnover).sort_values(ascending=False)\n",
    "    top_turnover = turnover_series.head(top_n)\n",
    "\n",
    "    result = [{\"name\": name, \"total_turnover\": int(turn)} for name, turn in top_turnover.items()]\n",
    "    json_data = json.dumps(result, indent=2)\n",
    "    save_json(json_data, f\"turnover_parks_{category}.json\")\n",
    "    return json_data\n",
    "\n",
    "# Funzioni per generare tutti i JSON per ogni categoria\n",
    "def generate_all_json_for_category(df, category):\n",
    "    generate_top_bottom_occupancy(df, category)\n",
    "    generate_weekday_occupancy(df, category)\n",
    "    generate_hourly_occupancy(df, category)\n",
    "    generate_weekend_holiday_comparison(df, category)\n",
    "    generate_turnover_parks(df, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7ebdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_json_for_category(parks_park_filled, 'park')\n",
    "generate_all_json_for_category(parks_bike_filled, 'bike')\n",
    "generate_all_json_for_category(zones_blu_filled, 'zones_blu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17f13232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Conversione sicura dei timestamp (timezone-aware)\n",
    "parks_park_filled['timestamp'] = pd.to_datetime(parks_park_filled['timestamp'], utc=True)\n",
    "parks_bike_filled['timestamp'] = pd.to_datetime(parks_bike_filled['timestamp'], utc=True)\n",
    "\n",
    "# Creo la cartella di output\n",
    "output_dir = DEST + \"calendar_heatmaps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Funzione per creare heatmap calendario mese/giorno\n",
    "def create_monthly_calendar_heatmaps(df, value_column, title_prefix):\n",
    "    df = df.copy()\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "    df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "    df['day'] = pd.to_datetime(df['date']).dt.day\n",
    "    df['weekday'] = pd.to_datetime(df['date']).dt.weekday\n",
    "\n",
    "    # Ciclo su ogni mese\n",
    "    for (year, month), group in df.groupby(['year', 'month']):\n",
    "        month_name = calendar.month_name[month]\n",
    "        pivot = group.pivot_table(index='weekday', columns='day', values=value_column, aggfunc='mean')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        sns.heatmap(pivot, cmap=\"YlGnBu\", ax=ax, cbar_kws={'label': value_column}, linewidths=0.5)\n",
    "        ax.set_title(f'{title_prefix} - {month_name} {year}', fontsize=16)\n",
    "        ax.set_xlabel('Giorno del mese')\n",
    "        ax.set_ylabel('Giorno della settimana')\n",
    "        ax.set_yticklabels(['Lun', 'Mar', 'Mer', 'Gio', 'Ven', 'Sab', 'Dom'], rotation=0)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Salvataggio SVG\n",
    "        filename = f\"{title_prefix.replace(' ', '_')}_{year}_{month:02d}.svg\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, format='png')\n",
    "        plt.close()\n",
    "\n",
    "# Creazione heatmap globali (auto e bici)\n",
    "create_monthly_calendar_heatmaps(parks_park_filled, 'percent_occupied', 'Parcheggi_Auto')\n",
    "create_monthly_calendar_heatmaps(parks_bike_filled, 'percent_occupied', 'Ciclobox_Bici')\n",
    "\n",
    "# Creazione heatmap per ogni parcheggio\n",
    "for name, group in parks_park_filled.groupby('name'):\n",
    "    safe_name = name.replace(' ', '_').replace('/', '_')\n",
    "    create_monthly_calendar_heatmaps(group, 'percent_occupied', f'Parcheggio_Auto_{safe_name}')\n",
    "\n",
    "# Creazione heatmap per ogni ciclobox\n",
    "for name, group in parks_bike_filled.groupby('name'):\n",
    "    safe_name = name.replace(' ', '_').replace('/', '_')\n",
    "    create_monthly_calendar_heatmaps(group, 'percent_occupied', f'Ciclobox_Bici_{safe_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc25a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported Type RangeIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# CREAZIONE HEATMAP GLOBALI\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mcreate_github_style_calendar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparks_park_filled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpercent_occupied\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mParcheggi Auto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m create_github_style_calendar(parks_bike_filled, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_occupied\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCiclobox Bici\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# CREAZIONE HEATMAP PER OGNI PARCHEGGIO\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m, in \u001b[0;36mcreate_github_style_calendar\u001b[0;34m(df, value_column, title_prefix)\u001b[0m\n\u001b[1;32m     17\u001b[0m daily_avg \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m)[value_column]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Seleziono solo mesi con dati\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m available_months \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_avg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_period\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m available_months:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Filtra i dati del mese\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     start_date \u001b[38;5;241m=\u001b[39m month\u001b[38;5;241m.\u001b[39mstart_time\u001b[38;5;241m.\u001b[39mdate()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:6035\u001b[0m, in \u001b[0;36mSeries.to_period\u001b[0;34m(self, freq, copy)\u001b[0m\n\u001b[1;32m   5991\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5992\u001b[0m \u001b[38;5;124;03mConvert Series from DatetimeIndex to PeriodIndex.\u001b[39;00m\n\u001b[1;32m   5993\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6032\u001b[0m \u001b[38;5;124;03mPeriodIndex(['2023', '2024', '2025'], dtype='period[Y-DEC]')\u001b[39;00m\n\u001b[1;32m   6033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, DatetimeIndex):\n\u001b[0;32m-> 6035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported Type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6037\u001b[0m new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write())\n\u001b[1;32m   6038\u001b[0m new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_period(freq\u001b[38;5;241m=\u001b[39mfreq)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported Type RangeIndex"
     ]
    }
   ],
   "source": [
    "\n",
    "import dayplot as dp\n",
    "import locale\n",
    "\n",
    "\n",
    "# CONFIGURAZIONE ITALIANA\n",
    "try:\n",
    "    locale.setlocale(locale.LC_TIME, 'it_IT.UTF-8')\n",
    "except locale.Error:\n",
    "    print(\"Locale italiano non disponibile, uso quello di default.\")\n",
    "\n",
    "def create_github_style_calendar(df, value_column, title_prefix):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['timestamp'].dt.date)  # sempre datetime\n",
    "\n",
    "    # Calcolo la media giornaliera\n",
    "    daily_avg = df.groupby('date')[value_column].mean().reset_index()\n",
    "\n",
    "    # Conversione corretta\n",
    "    daily_avg['date'] = pd.to_datetime(daily_avg['date'])\n",
    "    available_months = daily_avg['date'].dt.to_period('M').unique()\n",
    "\n",
    "    for month in available_months:\n",
    "        # Filtra i dati del mese\n",
    "        start_date = month.start_time\n",
    "        end_date = month.end_time\n",
    "\n",
    "        month_data = daily_avg[(daily_avg['date'] >= start_date) & (daily_avg['date'] <= end_date)]\n",
    "\n",
    "        if month_data.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 4), dpi=300)\n",
    "        import dayplot as dp\n",
    "        dp.calendar(\n",
    "            dates=month_data['date'],\n",
    "            values=month_data[value_column],\n",
    "            cmap='YlGnBu',\n",
    "            vmin=0,\n",
    "            vmax=100,\n",
    "            ax=ax,\n",
    "            language='it',\n",
    "        )\n",
    "        ax.set_title(f\"{title_prefix} - {month.strftime('%B %Y')}\", fontsize=14)\n",
    "\n",
    "        # Salva il file\n",
    "        safe_title = title_prefix.replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{safe_title}_{month.year}_{month.month:02d}.svg\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        fig.savefig(filepath, format='svg')\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# CREAZIONE HEATMAP GLOBALI\n",
    "create_github_style_calendar(parks_park_filled, 'percent_occupied', 'Parcheggi Auto')\n",
    "create_github_style_calendar(parks_bike_filled, 'percent_occupied', 'Ciclobox Bici')\n",
    "\n",
    "# CREAZIONE HEATMAP PER OGNI PARCHEGGIO\n",
    "for name, group in parks_park_filled.groupby('name'):\n",
    "    safe_name = name.replace(' ', '_').replace('/', '_')\n",
    "    create_github_style_calendar(group, 'percent_occupied', f'Parcheggio_Auto_{safe_name}')\n",
    "\n",
    "# CREAZIONE HEATMAP PER OGNI CICLOBOX\n",
    "for name, group in parks_bike_filled.groupby('name'):\n",
    "    safe_name = name.replace(' ', '_').replace('/', '_')\n",
    "    create_github_style_calendar(group, 'percent_occupied', f'Ciclobox_Bici_{safe_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
